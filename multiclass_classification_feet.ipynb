{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d688dd17",
   "metadata": {},
   "source": [
    "# Multiclass Classifier to Predict Feet Items \n",
    "The objective of this notebook is to build a machine learning model that predicts feet items (shoe & sock combination) by leveraging random forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80261010",
   "metadata": {},
   "source": [
    "## 1. Importing required modules and loading data file\n",
    "In the first section of this notebook, we have to import all neccessary libraries, required modules and essential packages. After running them in this cell, we will be able to use them in subsequent cells throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e92572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # for displaying plots\n",
    "import seaborn as sns; sns.set() # plotting package for histograms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import metrics\n",
    "feets = pd.read.csv('input_data.csv') #provide the path to the data file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590a586c",
   "metadata": {},
   "source": [
    "## 2. Exploratory data analysis (EDA) and data cleaning\n",
    "In this section, it is essential to conduct the initial investigations on data to discover the patterns, identify outliers and spot noises with the help of summary statistics and graphical representations. The goal of EDA is to summarize the important characteristics of data in order to gain better understanding of the dataset and prepare data for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10185d57",
   "metadata": {},
   "source": [
    "### The first step in EDA is to get familiar with dataset\n",
    "After importing the dataset, we get a quick glance at a data and check the shape of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b79f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the first view of a dataset & check if data has been read into a dataframe object\n",
    "feets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f15e8541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out the total number of rows and columns in the dataset \n",
    "feets.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2022b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out the columns & their corresponding data types & checking if they contain null values\n",
    "feets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e55bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gain basic overview of columns in a dataset\n",
    "feets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce716e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change datatype if neccessary\n",
    "feets = feets.astype({\"Column 1\": desired type, \"Column 2\": desired type}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9666a2",
   "metadata": {},
   "source": [
    "### The second step is to get rid of redundant columns (if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad7eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "feets = feets.drop (['column_1', 'column_2'], axis = 1)\n",
    "feets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d70456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are duplicates and handle them accordingly\n",
    "feets. duplicated().sum()\n",
    "feets.drop.duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e153a4",
   "metadata": {},
   "source": [
    "Use the [this](https://towardsdatascience.com/7-ways-to-handle-missing-values-in-machine-learning-1a6326adf79e) tutorial for handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e181aa33",
   "metadata": {},
   "source": [
    "### The third step is to conduct descriptive statistics of a datatset.\n",
    "We call the describe() function to gain insights into the shape of each attribute by creating summary statistics by viewing central tendency, mean, median, standard deviation, percentile and max values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e365210",
   "metadata": {},
   "outputs": [],
   "source": [
    "feets.describe() #for the summary of numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc6ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feets['column_name'].value_counts()#for the summary of categorical data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc162bd",
   "metadata": {},
   "source": [
    "### The fourth step is to create a bar chart.\n",
    "Plotting the bar chart will show the class distribution of the independent variables (x axis distinct items, y axis frequency). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ca947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bar charts - for categorical data\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([])\n",
    "variable_a = ['category_1', 'categroy_2', 'categroy_3']\n",
    "variable_b = [32, 23, 3, 6]#should be replaced\n",
    "ax.bar (variable_a, variable_b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7aaf43",
   "metadata": {},
   "source": [
    "### This fifth step is to plot histograms/kernel density plots for each variables. \n",
    "This visualization technique will enable us to see how frequently data in each class occur in the dataset. Namely, it will graphically show the frequency of different data points in the dataset, location of the center of data, the spread of dataset, skewness of dataset and presence of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2193add",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histograms - for quantitative data\n",
    "sns.distplot(example_data[‘column’], kde = False/True).set_title(‘Title’)\n",
    "#kde - by default always includes are density plot. if it visually distracts, you can set the parameter kde into false. \n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17c52e9",
   "metadata": {},
   "source": [
    "### The sixth step in this section is to create a boxplot for input features.\n",
    "This way, we get an even better idea about the centre of the distribution as well as verify potential outliers that we have detected in histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b752f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a box plot for input features\n",
    "sns.boxplot(x = 'column_name', y = 'column_name', data = feets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd716c34",
   "metadata": {},
   "source": [
    "### The last step of this section checks the correlation between features in a dataset by calling df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415ee0c7",
   "metadata": {},
   "source": [
    " \n",
    " It calculates the correlation between features pairwise excluding null values. Thus, we will gain understanding on one or multiple attributes that might be dependent on another attribute or a cause for another attribute.Feature correlations matter in predicting one attribute from another.\n",
    "\n",
    "The cell computes a correlation matrix (Max's note: be mindful of the type of correlation and the scale level of the variable, pearson vs. spearman correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a02c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "feets.corr()# to reveal whether correlation is positive, negative\n",
    "#or non-existent. \n",
    "#Read more on correlation matrix to interpret Max's note above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870a5b7d",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Data pre-processing\n",
    "In this section we will preapre machine learning data by splitting data to three datatsets: train-set, validation- and test-sets. The third set is important to test the final performance of the model. It is used only  on the fine-tuned model. Once it is used, it loses its \"value\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1391461a",
   "metadata": {},
   "source": [
    "### In the first of this section, we will create machine learning datatsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1de51a",
   "metadata": {},
   "source": [
    "Create a train/validation/test split (e.g., 50%, 25%, 25% according to Hastie et al. (2009, p. 222)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64e7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for our model we use such output data as temperature, sex, heat perception and weather state\n",
    "X = feet[['temp','temperature', 'sex', 'heat perception' 'weather state']]\n",
    "y = feet['feet_label']\n",
    "\n",
    "# default is 75% / 25% train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec22a4",
   "metadata": {},
   "source": [
    "### Select the algorithms and create a classifier object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca297366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find corresponding code that would create classifier object for the \n",
    "#following algorithms\n",
    "knn = \n",
    "dummy_classifier=\n",
    "random_forest_classifier ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cb6ae8",
   "metadata": {},
   "source": [
    "## 4. Model training\n",
    "In this section, we will feed the ML algorithms with data to help identify and learn good values for all attributed involved. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130543c0",
   "metadata": {},
   "source": [
    "### Dummy Classifiers\n",
    "DummyClassifier is a classifier that makes predictions using simple rules, which can be useful as a baseline for comparison against actual classifiers, especially with imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative class (0) is most frequent\n",
    "dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\n",
    "# Therefore the dummy 'most_frequent' classifier always predicts class 0\n",
    "y_dummy_predictions = dummy_majority.predict(X_test)\n",
    "\n",
    "y_dummy_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaf4902",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbours Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57412114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alter the following code to meet our needs\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2,\n",
    "                                                   random_state=0)\n",
    "\n",
    "plot_two_class_knn(X_train, y_train, 1, 'uniform', X_test, y_test)\n",
    "plot_two_class_knn(X_train, y_train, 3, 'uniform', X_test, y_test)\n",
    "plot_two_class_knn(X_train, y_train, 11, 'uniform', X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7981462a",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "Adapt the following example code from Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001. For documentation click [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7571f0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X, y)\n",
    "RandomForestClassifier(...)\n",
    "print(clf.predict([[0, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7ed268",
   "metadata": {},
   "source": [
    "### Rule-based algorithm \n",
    "The rule-based model file will be incorporated here once it is finalized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa819fd",
   "metadata": {},
   "source": [
    "### Training the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "827975e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d312c57f",
   "metadata": {},
   "source": [
    "## 5. Model evaluation \n",
    "#### (more research is needed to define the relevant metrics)\n",
    "In this step we will use corresponding metrics to test the ability of multi-class classifier by comparing the performance of different models and eventually, analysing the best peforming model my tuning different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4746e7e",
   "metadata": {},
   "source": [
    "### Use the trained classifier model to classify new objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f93afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new object classification on the \"test set\" as defined by Hastie et al. above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d956fdd",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "This metric will be directly computed from the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86daa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#algorithm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d86c085",
   "metadata": {},
   "source": [
    "### Multi-class confusion matrix\n",
    "Confusion Matrix is used to know the performance of a Machine learning classification. It is represented in a matrix form. [Confusion Matrix](https://www.analyticsvidhya.com/blog/2021/06/confusion-matrix-for-multi-class-classification/#:~:text=Confusion%20Matrix%20is%20used%20to,between%20Actual%20and%20predicted%20values.&text=Confusion%20Matrix%20has%204%20terms,and%20False%20Negative(FN).) gives a comparison between Actual and predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b9a34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e631c58f",
   "metadata": {},
   "source": [
    "### Precision and recall\n",
    "Precision quantifies the number of positive class predictions that actually belong to the positive class. Recall quantifies the number of positive class predictions made out of all positive examples in the dataset. [For more read the following blog](https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d919b5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e5d41f",
   "metadata": {},
   "source": [
    "### K-fold cross-validation\n",
    "In this step, we will leverage k-fold cross validation  to estimate the skill of the model on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aec558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54e6671",
   "metadata": {},
   "source": [
    "## 6. Model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e011927",
   "metadata": {},
   "source": [
    "### Deploying the output predictions for the digital product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c7019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some code (look this up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c13d2",
   "metadata": {},
   "source": [
    "Further steps: save the model as a file, and create an .xls file with the testset (as defined by Hastie et al., 2009) and corresponding predictions in an additional column Details to be aligned with WD team (e.g., Flask interface)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
